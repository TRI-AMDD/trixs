{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Benchmark the CNNs based on HP opt done elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T04:47:48.187381Z",
     "start_time": "2019-08-17T04:47:48.183031Z"
    }
   },
   "outputs": [],
   "source": [
    "target_elements_groups=[('Co','O'),('Fe','O'),('V','O'),('Cu','O'),\n",
    "                        ('Ni','O'),('Cr','O'),('Mn','O'),('Ti','O')]\n",
    "target_metals = set(['Co','Ni','Fe','Cr','V','Mn','Cu','Ti'])\n",
    "target_elements_sets =[set(pair) for pair in target_elements_groups]\n",
    "storage_directory = '/Users/steventorrisi/Documents/TRIXS/data/MP_OQMD_combined'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T04:48:14.804502Z",
     "start_time": "2019-08-17T04:48:14.781221Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Machine learning:\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, \\\n",
    " MaxPooling1D, Flatten\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import json\n",
    "from trixs.machine_learning.benchmarks import plot_coordination_confusion_matrix\n",
    "from trixs.machine_learning.util import onehot_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T04:48:15.003829Z",
     "start_time": "2019-08-17T04:48:14.992592Z"
    }
   },
   "outputs": [],
   "source": [
    "def classification_model(x_train, y_train, x_val, y_val, params):\n",
    "\n",
    "    # Params is the vessel that will carry the optimzation parameters\n",
    "    dropout = params['dropout']\n",
    "    act = params['activation_function']\n",
    "    optimizer = params['optimizer']\n",
    "\n",
    "    #layers = params['layers']\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN vs. MLP.\n",
    "    if params['cnn']:\n",
    "        x_train = np.expand_dims(x_train, axis=-1)\n",
    "        x_val = np.expand_dims(x_val, axis=-1)\n",
    "        model.add(Conv1D(params['kernel'],\n",
    "                         params['n_filters'],\n",
    "                         strides=params['strides'], padding='valid',\n",
    "                         activation=act,\n",
    "                         input_shape=(x_train.shape[1], 1)))\n",
    "        model.add(MaxPooling1D(pool_size=params['pool_size'],\n",
    "                               strides=None, padding='valid'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(params['layer0'], activation=act))\n",
    "    else:\n",
    "        model.add(Dense(params['layer0'], activation=act,\n",
    "                        input_shape=(x_train.shape[1],)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    # note the change here relative to the regresion problem\n",
    "    model.add(Dense(params['layer1'], activation=act))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(params['layer2'], activation=act))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss=params['loss_function'], optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                      batch_size=params['batch_size'],\n",
    "                      epochs=params['epochs'],\n",
    "                      validation_data=[x_val, y_val],\n",
    "                      verbose=0,\n",
    "                      shuffle=True)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T04:52:30.998852Z",
     "start_time": "2019-08-17T04:52:30.994948Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'layer0': [9030],\n",
    "    'layer1':[60], \n",
    "    'layer2':[30],\n",
    "    'dropout': [0.1],\n",
    "    'activation_function': 'relu',\n",
    "    'optimizer': ['adam'],\n",
    "    'cnn': [True],\n",
    "    'kernel': [8],\n",
    "    'n_filters': [10],\n",
    "    'strides': [1],\n",
    "    'pool_size': [2],\n",
    "    'loss_function': ['categorical_crossentropy'],\n",
    "    'batch_size': [32],\n",
    "    'epochs': [30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T04:52:35.769312Z",
     "start_time": "2019-08-17T04:52:31.589448Z"
    }
   },
   "outputs": [],
   "source": [
    "data_by_pair = {pair:[] for pair in target_elements_groups}\n",
    "for pair in target_elements_groups:\n",
    "    file_target = storage_directory+'/{}_{}_XY.json'.format(pair[0],pair[1])\n",
    "    with open(file_target,'r') as f:\n",
    "        data_by_pair[pair] = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "data_by_pair = {pair:[] for pair in target_elements_groups}\n",
    "\n",
    "models_by_pair = {pair:None for pair in target_elements_groups}\n",
    "x_valid_by_pair = {pair:None for pair in target_elements_groups}\n",
    "y_valid_by_pair = {pair:None for pair in target_elements_groups}\n",
    "\n",
    "for pair in tqdm_notebook(target_elements_groups[0:1]):\n",
    "    target_file = storage_directory + '/{}_{}_XY.json'.format(pair[0],pair[1])\n",
    "    with open(target_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            cur_data = json.loads(line)\n",
    "            if cur_data.get('one_hot_coord'):\n",
    "                data_by_pair[pair].append(cur_data)\n",
    "    \n",
    "    X = []\n",
    "    Y = []    \n",
    "    for point in data_by_pair[pair]:\n",
    "        X.append(np.array(point['mu']).reshape(100))\n",
    "        Y.append(np.array(point['one_hot_coord']))\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "      train_test_split(X, Y, test_size=0.1,\n",
    "                   random_state=42)\n",
    "    x_train, x_valid, y_train, y_valid = \\\n",
    "      train_test_split(x_train, y_train, test_size=0.1,\n",
    "                       random_state=42)\n",
    "    \n",
    "    \n",
    "    train_feature_mean = np.mean(np.mean(x_train, axis=0))\n",
    "    train_feature_std = np.mean(np.std(x_train, axis=0))\n",
    "\n",
    "    valid_feature_mean = np.mean(np.mean(x_valid, axis=0))\n",
    "    valid_feature_std = np.mean(np.std(x_valid, axis=0))\n",
    "\n",
    "    test_feature_mean = np.mean(np.mean(x_test, axis=0))\n",
    "    test_feature_std = np.mean(np.std(x_test, axis=0))\n",
    "\n",
    "\n",
    "    print(\"~~~~~~~~~~~~ Mean +/- Stdev before ~~~~~~~~~~~~~~\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Train features %.03f +/- %.03f\"\n",
    "          % (train_feature_mean, train_feature_std))\n",
    "    print(\"Valid features %.03f +/- %.03f\"\n",
    "          % (valid_feature_mean, valid_feature_std))\n",
    "    print(\"Test features  %.03f +/- %.03f\"\n",
    "          % (test_feature_mean, test_feature_std))\n",
    "\n",
    "\n",
    "    # Generate a feature and target scaler\n",
    "    feature_scaler = StandardScaler().fit(x_train)\n",
    "\n",
    "    # Utilize that scaler on the datasets\n",
    "\n",
    "    x_train = feature_scaler.transform(x_train)\n",
    "    x_valid = feature_scaler.transform(x_valid)\n",
    "    x_test = feature_scaler.transform(x_test)\n",
    "\n",
    "    train_feature_mean = np.mean(np.mean(x_train, axis=0))\n",
    "    train_feature_std = np.mean(np.std(x_train, axis=0))\n",
    "\n",
    "    valid_feature_mean = np.mean(np.mean(x_valid, axis=0))\n",
    "    valid_feature_std = np.mean(np.std(x_valid, axis=0))\n",
    "\n",
    "    test_feature_mean = np.mean(np.mean(x_test, axis=0))\n",
    "    test_feature_std = np.mean(np.std(x_test, axis=0))\n",
    "\n",
    "\n",
    "    print(\"~~~~~~~~~~~~~ Mean +/- Stdev after ~~~~~~~~~~~~~~\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Train features %.03f +/- %.03f\"\n",
    "          % (train_feature_mean, train_feature_std))\n",
    "    print(\"Valid features %.03f +/- %.03f\"\n",
    "          % (valid_feature_mean, valid_feature_std))\n",
    "    print(\"Test features  %.03f +/- %.03f\"\n",
    "          % (test_feature_mean, test_feature_std))\n",
    "    \n",
    "        \n",
    "    _, the_model =  classification_model(x_train, y_train, x_valid, y_valid, params)\n",
    "    \n",
    "    \n",
    "    models_by_pair[pair] = the_model\n",
    "    x_valid_by_pair[pair] = x_valid\n",
    "    y_valid_by_pair[pair] = y_valid\n",
    "    \n",
    "    y_attempt= [np.argmax(y) for y in the_model.predict(x_valid)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
