{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Perform a hyperparameter sweep over a set of CNN parameters for regression of Bader charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T23:23:45.920689Z",
     "start_time": "2019-08-12T23:23:45.880793Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5421,
     "status": "ok",
     "timestamp": 1565227162378,
     "user": {
      "displayName": "Matthew Carbone",
      "photoUrl": "https://lh5.googleusercontent.com/-ua_pIWuZups/AAAAAAAAAAI/AAAAAAAACFE/asTymiafjII/s64/photo.jpg",
      "userId": "02950552987981797974"
     },
     "user_tz": 240
    },
    "id": "FYBAxGhpRUAA",
    "outputId": "101032ed-67ac-457b-c910-1bb5ace92156"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Machine learning:\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, \\\n",
    " MaxPooling1D, Flatten\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import json\n",
    "import talos as ta\n",
    "\n",
    "from trixs.machine_learning.util import onehot_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T23:23:46.712099Z",
     "start_time": "2019-08-12T23:23:45.933024Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "piQbmjWMRl6k"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for line in open('./Ti_O_XY.json', 'r'):\n",
    "    data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T23:23:46.727424Z",
     "start_time": "2019-08-12T23:23:46.715539Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5361,
     "status": "ok",
     "timestamp": 1565227163021,
     "user": {
      "displayName": "Matthew Carbone",
      "photoUrl": "https://lh5.googleusercontent.com/-ua_pIWuZups/AAAAAAAAAAI/AAAAAAAACFE/asTymiafjII/s64/photo.jpg",
      "userId": "02950552987981797974"
     },
     "user_tz": 240
    },
    "id": "wsZ7omaAR3jh",
    "outputId": "6b15d5b5-838c-4c53-964d-48319ee7cbd7"
   },
   "outputs": [],
   "source": [
    "print(\"Total of %i data points\" % len(data))\n",
    "print(\"Keys are %a\" % data[0].keys())\n",
    "\n",
    "# Using mu here not mu0\n",
    "all_feature_lens = [len(data[ii]['mu']) == len(data[0]['mu'])\n",
    "                    for ii in range(len(data))]\n",
    "print(\"All features same length: %s\" % np.all(np.array(all_feature_lens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdJTLEWWXyzL"
   },
   "source": [
    "## Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T23:23:46.821643Z",
     "start_time": "2019-08-12T23:23:46.731220Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1565227403439,
     "user": {
      "displayName": "Matthew Carbone",
      "photoUrl": "https://lh5.googleusercontent.com/-ua_pIWuZups/AAAAAAAAAAI/AAAAAAAACFE/asTymiafjII/s64/photo.jpg",
      "userId": "02950552987981797974"
     },
     "user_tz": 240
    },
    "id": "qThZv-YCTJbu",
    "outputId": "e3a88b05-1916-430f-e4c6-4ab64839db2b"
   },
   "outputs": [],
   "source": [
    "# Only take 4, 5 and 6 coordinated:\n",
    "to_ignore = [ii for ii in range(len(data)) if data[ii]['coordination'] not in [4, 5, 6]]\n",
    "features = np.array([data[ii]['mu'] for ii in range(len(data)) if ii not in to_ignore])\n",
    "\n",
    "# Minus 4 so that 6 - 4 = 2, 5 - 4 = 1 and 4 - 4 = 0; ensures that my one-hot\n",
    "# decoder/encoder system can work properly!\n",
    "_targets = np.array([data[ii]['coordination'] - 4 for ii in range(len(data)) if ii not in to_ignore])\n",
    "targets = onehot_reverse(_targets)\n",
    "print(features.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T23:23:49.550719Z",
     "start_time": "2019-08-12T23:23:49.540022Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3064,
     "status": "ok",
     "timestamp": 1565227163160,
     "user": {
      "displayName": "Matthew Carbone",
      "photoUrl": "https://lh5.googleusercontent.com/-ua_pIWuZups/AAAAAAAAAAI/AAAAAAAACFE/asTymiafjII/s64/photo.jpg",
      "userId": "02950552987981797974"
     },
     "user_tz": 240
    },
    "id": "_64SdfQ-TQpL",
    "outputId": "bf1ebaa3-59e7-4064-e0ea-e22d9de03f99"
   },
   "outputs": [],
   "source": [
    "# Generate train/validate/test splits\n",
    "RANDSTATE=555\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "  train_test_split(features, targets, test_size=0.1,\n",
    "                   random_state=RANDSTATE)\n",
    "x_train, x_valid, y_train, y_valid = \\\n",
    "  train_test_split(x_train, y_train, test_size=0.1,\n",
    "                   random_state=RANDSTATE)\n",
    "print(\"training    %s ~ %s\"\n",
    "      % ((x_train.shape), (y_train.shape)))\n",
    "print(\"validation  %s ~ %s\"\n",
    "      % ((x_valid.shape), (y_valid.shape)))\n",
    "print(\"testing     %s ~ %s\"\n",
    "      % ((x_test.shape), (y_test.shape)))\n",
    "\n",
    "# Want to normalize the targets first\n",
    "y_train = y_train/np.max(y_train, axis=1, keepdims=True)\n",
    "y_valid = y_valid/np.max(y_valid, axis=1, keepdims=True)\n",
    "y_test = y_test/np.max(y_test, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T23:23:50.305365Z",
     "start_time": "2019-08-12T23:23:50.285680Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2096,
     "status": "ok",
     "timestamp": 1565227163161,
     "user": {
      "displayName": "Matthew Carbone",
      "photoUrl": "https://lh5.googleusercontent.com/-ua_pIWuZups/AAAAAAAAAAI/AAAAAAAACFE/asTymiafjII/s64/photo.jpg",
      "userId": "02950552987981797974"
     },
     "user_tz": 240
    },
    "id": "WvUXS2y8dR9e",
    "outputId": "3d77c94e-deee-4a58-fd51-e24f18ca5e91"
   },
   "outputs": [],
   "source": [
    "# The values of the input features vary wildly between different\n",
    "# feature types, over different orders of magnitude. Thus, we\n",
    "# must scale each feature to zero mean and unit variance to\n",
    "# avoid problematic issues during training. We fit only on the\n",
    "# training data, and execute that same scaling on the validation\n",
    "# and testing data to ensure we do not accidentally bias the\n",
    "# latter two data sets. Sanity check: printing out the feature/\n",
    "# target means before and after (averaged over all features/\n",
    "# targets just to get an idea).\n",
    "\n",
    "train_feature_mean = np.mean(np.mean(x_train, axis=0))\n",
    "train_feature_std = np.mean(np.std(x_train, axis=0))\n",
    "\n",
    "valid_feature_mean = np.mean(np.mean(x_valid, axis=0))\n",
    "valid_feature_std = np.mean(np.std(x_valid, axis=0))\n",
    "\n",
    "test_feature_mean = np.mean(np.mean(x_test, axis=0))\n",
    "test_feature_std = np.mean(np.std(x_test, axis=0))\n",
    "\n",
    "\n",
    "print(\"~~~~~~~~~~~~ Mean +/- Stdev before ~~~~~~~~~~~~~~\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"Train features %.03f +/- %.03f\"\n",
    "      % (train_feature_mean, train_feature_std))\n",
    "print(\"Valid features %.03f +/- %.03f\"\n",
    "      % (valid_feature_mean, valid_feature_std))\n",
    "print(\"Test features  %.03f +/- %.03f\"\n",
    "      % (test_feature_mean, test_feature_std))\n",
    "\n",
    "\n",
    "# Generate a feature and target scaler\n",
    "feature_scaler = StandardScaler().fit(x_train)\n",
    "\n",
    "# Utilize that scaler on the datasets\n",
    "\n",
    "x_train = feature_scaler.transform(x_train)\n",
    "x_valid = feature_scaler.transform(x_valid)\n",
    "x_test = feature_scaler.transform(x_test)\n",
    "\n",
    "train_feature_mean = np.mean(np.mean(x_train, axis=0))\n",
    "train_feature_std = np.mean(np.std(x_train, axis=0))\n",
    "\n",
    "valid_feature_mean = np.mean(np.mean(x_valid, axis=0))\n",
    "valid_feature_std = np.mean(np.std(x_valid, axis=0))\n",
    "\n",
    "test_feature_mean = np.mean(np.mean(x_test, axis=0))\n",
    "test_feature_std = np.mean(np.std(x_test, axis=0))\n",
    "\n",
    "print(\"~~~~~~~~~~~~~ Mean +/- Stdev after ~~~~~~~~~~~~~~\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"Train features %.03f +/- %.03f\"\n",
    "      % (train_feature_mean, train_feature_std))\n",
    "print(\"Valid features %.03f +/- %.03f\"\n",
    "      % (valid_feature_mean, valid_feature_std))\n",
    "print(\"Test features  %.03f +/- %.03f\"\n",
    "      % (test_feature_mean, test_feature_std))\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T23:23:50.865383Z",
     "start_time": "2019-08-12T23:23:50.854604Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "cQ4DlTXwWRTO"
   },
   "outputs": [],
   "source": [
    "def classification_model(x_train, y_train, x_val, y_val, params):\n",
    "\n",
    "    # Params is the vessel that will carry the optimzation parameters\n",
    "    dropout = params['dropout']\n",
    "    act = params['activation_function']\n",
    "    optimizer = params['optimizer']\n",
    "\n",
    "    #layers = params['layers']\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN vs. MLP.\n",
    "    if params['cnn']:\n",
    "        x_train = np.expand_dims(x_train, axis=-1)\n",
    "        x_val = np.expand_dims(x_val, axis=-1)\n",
    "        model.add(Conv1D(params['kernel'],\n",
    "                         params['n_filters'],\n",
    "                         strides=params['strides'], padding='valid',\n",
    "                         activation=act,\n",
    "                         input_shape=(x_train.shape[1], 1)))\n",
    "        model.add(MaxPooling1D(pool_size=params['pool_size'],\n",
    "                               strides=None, padding='valid'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(params['layer0'], activation=act))\n",
    "    else:\n",
    "        model.add(Dense(params['layer0'], activation=act,\n",
    "                        input_shape=(x_train.shape[1],)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "\n",
    "    # note the change here relative to the regresion problem\n",
    "    model.add(Dense(params['layer1'], activation=act))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(params['layer2'], activation=act))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    model.compile(loss=params['loss_function'], optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                      batch_size=params['batch_size'],\n",
    "                      epochs=params['epochs'],\n",
    "                      validation_data=[x_val, y_val],\n",
    "                      verbose=0,\n",
    "                      shuffle=True)\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T01:25:38.975576Z",
     "start_time": "2019-08-13T01:25:38.970481Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oAwK_oFzd2vC"
   },
   "outputs": [],
   "source": [
    "# Steve, play around with this a bit! Also, I would be careful: the accuracy\n",
    "# is not a great metric here if you have major class imbalances! :)\n",
    "params = {\n",
    "    'layer0': [90,60,45, 30],\n",
    "    'layer1':[45,60,75], \n",
    "    'layer2':[15,30,45],\n",
    "    'dropout': [0.1],\n",
    "    'activation_function': ['relu'],\n",
    "    'optimizer': ['adam'],\n",
    "    'cnn': [True],\n",
    "    'kernel': [8,12,14],\n",
    "    'n_filters': [10,12],\n",
    "    'strides': [1,2],\n",
    "    'pool_size': [1,2,3],\n",
    "    'loss_function': ['categorical_crossentropy'],\n",
    "    'batch_size': [32],\n",
    "    'epochs': [30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T01:55:04.666303Z",
     "start_time": "2019-08-13T01:30:27.611685Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10765,
     "status": "ok",
     "timestamp": 1565229544725,
     "user": {
      "displayName": "Matthew Carbone",
      "photoUrl": "https://lh5.googleusercontent.com/-ua_pIWuZups/AAAAAAAAAAI/AAAAAAAACFE/asTymiafjII/s64/photo.jpg",
      "userId": "02950552987981797974"
     },
     "user_tz": 240
    },
    "id": "b1CWAAGVpDSb",
    "outputId": "7e4e8696-ab85-484a-cfab-93a84a141c33"
   },
   "outputs": [],
   "source": [
    "t = ta.Scan(x_train, y_train, x_val=x_valid, y_val=y_valid,\n",
    "            model=classification_model, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T23:24:05.465701Z",
     "start_time": "2019-08-12T23:24:05.404348Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1565229544726,
     "user": {
      "displayName": "Matthew Carbone",
      "photoUrl": "https://lh5.googleusercontent.com/-ua_pIWuZups/AAAAAAAAAAI/AAAAAAAACFE/asTymiafjII/s64/photo.jpg",
      "userId": "02950552987981797974"
     },
     "user_tz": 240
    },
    "id": "4jAEun7upzL4",
    "outputId": "b7a1beed-8c1a-4fb3-8b55-cf6cc1c0e3b0"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.width',500)\n",
    "print(t.data.sort_values(by='val_loss', ascending=True))\n",
    "display(HTML(t.data.sort_values(by='val_loss', ascending=True).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T02:02:56.792572Z",
     "start_time": "2019-08-13T02:02:56.741320Z"
    }
   },
   "outputs": [],
   "source": [
    "use_params = {\n",
    "    'layer0': 90,\n",
    "    'layer1':60, \n",
    "    'layer2':45,\n",
    "    'dropout': 0.1,\n",
    "    'activation_function': 'relu',\n",
    "    'optimizer': 'adam',\n",
    "    'cnn': True,\n",
    "    'kernel': 8,\n",
    "    'n_filters': 10,\n",
    "    'strides': 1,\n",
    "    'pool_size': 2,\n",
    "    'loss_function': 'categorical_crossentropy',\n",
    "    'batch_size': 32,\n",
    "    'epochs':25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T02:02:57.278281Z",
     "start_time": "2019-08-13T02:02:57.261942Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_layer_output_gradient_simple_target(model,X,target_output):\n",
    "    inputs = model.input\n",
    "    outputs = model.output\n",
    "    focus = model.output[0][target_output]\n",
    "    grad = K.gradients(focus, inputs)[0]\n",
    "    f = K.function([inputs], [outputs,grad])    \n",
    "    return f([X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T02:03:31.863368Z",
     "start_time": "2019-08-13T02:02:57.529619Z"
    }
   },
   "outputs": [],
   "source": [
    "_, the_model =  classification_model(x_train, y_train, x_valid, y_valid, use_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T02:03:31.890164Z",
     "start_time": "2019-08-13T02:03:31.865956Z"
    }
   },
   "outputs": [],
   "source": [
    "print(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T02:03:31.917542Z",
     "start_time": "2019-08-13T02:03:31.893188Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "one_hot_to_coord = {(1,0,0):0, (0,1,0):1, (0,0,1):2 }\n",
    "validation_by_coords = {(1,0,0):[], (0,1,0):[], (0,0,1):[] }\n",
    "grads_by_coords = {(1,0,0):[], (0,1,0):[], (0,0,1):[] }\n",
    "for i in tqdm_notebook(range(len(y_valid))):\n",
    "    cur_tup =  tuple([int(x) for x in y_valid[i]])\n",
    "    validation_by_coords[cur_tup].append(x_valid[i])\n",
    "    exs = x_valid[i].reshape(1,100,1)\n",
    "    grads_by_coords[cur_tup].append(get_layer_output_gradient_simple_target(the_model,\n",
    "                                                                            exs,\n",
    "                                                                            one_hot_to_coord[cur_tup])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T02:03:31.920702Z",
     "start_time": "2019-08-13T02:02:58.246Z"
    }
   },
   "outputs": [],
   "source": [
    "exs = x_valid.reshape(565,100,1)\n",
    "results, gradients = get_layer_output_gradient_simple_target(the_model, exs, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T02:03:31.922701Z",
     "start_time": "2019-08-13T02:02:58.491Z"
    }
   },
   "outputs": [],
   "source": [
    "class_results = np.argmax(results,axis=1)\n",
    "#print(class_results)\n",
    "print(class_results.shape)\n",
    "print(gradients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T02:03:31.924533Z",
     "start_time": "2019-08-13T02:02:58.741Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "X = np.linspace(0,1,100)\n",
    "#for _ in [0,1]:\n",
    "#for x in x_valid.reshape(565,100,1):   \n",
    "cmap = mpl.cm.get_cmap('bwr')\n",
    "norm = mpl.colors.Normalize(vmin=min(gradients[0]), vmax=max(gradients[0]))\n",
    "grad_colors = norm(gradients[0].reshape(100))\n",
    "mu = exs[0].reshape(100)\n",
    "colors = cmap(grad_colors)\n",
    "plt.plot(X,mu,zorder=-1)\n",
    "plt.colorbar()\n",
    "for i,x in enumerate(X):\n",
    "    plt.scatter(x,mu[i],color=colors[i])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T02:03:31.926537Z",
     "start_time": "2019-08-13T02:02:58.969Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from scipy.interpolate import interp1d\n",
    "x = np.linspace(0,1,100)\n",
    "for y in grads_by_coords[(1,0,0)]:\n",
    "    pass\n",
    "    #print(x,y)\n",
    "    #plt.plot(x,y.reshape(100),color='red')\n",
    "    \n",
    "megagrad = np.mean([y.reshape(100) for y in grads_by_coords[(1,0,0)]],axis=0)\n",
    "plt.plot(x,validation_by_coords[(0,1,0)][])\n",
    "plt.plot(x,megagrad)\n",
    "print(megagrad.shape)\n",
    "print(np.sum(megagrad,axis=0).shape)\n",
    "x_dens = np.linspace(0,1,300)\n",
    "print(x_dens.shape)\n",
    "print(x.shape)\n",
    "#megagrad_sum = np.sum(megagrad,axis=0)\n",
    "#f = interp1d(x,megagrad_sum)\n",
    "y_dens = f(x_dens)\n",
    "#plt.plot(x_dens,y_dens)\n",
    "#plt.plot(x_dens,gaussian_filter1d(y_dens,sigma=.05))\n",
    "#plt.plot(x,megagrad,lw=2)\n",
    "#for y in grads_by_coords[(0,1,0)]:\n",
    "    #print(x,y)\n",
    "#    plt.plot(x,y.reshape(100),color='blue')\n",
    "    \n",
    "#for y in grads_by_coords[(0,0,1)]:\n",
    "    #print(x,y)\n",
    "#    plt.plot(x,y.reshape(100),color='green')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hp_tuning_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
