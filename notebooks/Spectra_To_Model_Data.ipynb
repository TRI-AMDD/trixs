{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Process Data for Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some parameters which will affect the way the notebook executes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_aug_minority_classes_only = False\n",
    "\n",
    "# Write the maximum normalized spectra (set max val=1) \n",
    "# instead of the feff normalized spectra.\n",
    "# This matters more for the polynomial vectors, which were processed elsewhere.\n",
    "# This will DIRECTLY write the max-normalized mu to the .npy files. You can also normalize them later \n",
    "# per your use case (which is what Train_Run_Models does).\n",
    "use_max_normalized = False\n",
    "norm_str = 'max' if use_max_normalized else 'feff'\n",
    "\n",
    "# Flag to remove spectra which have a high error once fit to polynomials. This was used in the publication.\n",
    "drop_poly_problems = True\n",
    "\n",
    "\n",
    "# Use augmented data from the processed spectra on the *training set*.\n",
    "# We did NOT use this flag in the publication, as we found it interfered with the interpretability.\n",
    "# You are welcome to experiment, though!\n",
    "data_aug = False            \n",
    "\n",
    "# Augmented data in the training set includes strech / squeeze (dilation of the spectrum by 5%)\n",
    "# and pm 1 / 2, which shift the domain by 1 or 2 eV respectively\n",
    "\n",
    "# Over-sample in your data set minority classes, randomly with replacement.\n",
    "# Done only for pointwise models.\n",
    "oversampling = True\n",
    "\n",
    "# Random seed used for all train_test splits\n",
    "rseed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T20:31:41.671082Z",
     "start_time": "2019-10-18T20:31:33.060018Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "from pymatgen.core import Structure\n",
    "from pymatgen.analysis.structure_matcher import StructureMatcher, ElementComparator\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#sys.path.append(os.path.join(os.getcwd(), '..'))  # TRIXS path if not automatically detected\n",
    "from trixs.spectra.core import XAS_Spectrum, XAS_Collation\n",
    "from trixs.spectra.util import NumpyEncoder\n",
    "from trixs.machine_learning.benchmarks import precision_recall_matrix, confusion_dict\n",
    "from trixs.spectra.spectrum_featurize import polynomialize_by_idx, gauge_polynomial_error\n",
    "\n",
    "figure_write_folder = \"./gen_figures_feffnorm\" if use_max_normalized else './gen_figures_maxnorm'\n",
    "storage_directory = './spectral_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define domains which will be used for x-axis labels later, as well as define the elements which will be imported for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T20:31:41.687034Z",
     "start_time": "2019-10-18T20:31:41.673251Z"
    }
   },
   "outputs": [],
   "source": [
    "target_elements_groups=[('Ti','O'),('V','O'),('Cr','O'),\n",
    "                        ('Mn','O'),('Fe','O'),('Co','O'),\n",
    "                        ('Ni','O'),('Cu','O')]\n",
    "\n",
    "x_domains = {  ('Co','O'):  np.linspace(7713.5, 7765.83,100),\n",
    "               ('Fe','O'): np.linspace(7115.0, 7167.764,100),\n",
    "               ('V','O'):  np.linspace(5468.0, 5520.631,100),\n",
    "               ('Cu','O'): np.linspace( 8987.5, 9039.712,100),\n",
    "               ('Ni','O'): np.linspace( 8336.5 ,8388.723,100),\n",
    "               ('Cr','O'): np.linspace(5993.1, 6045.686,100),\n",
    "               ('Mn','O'): np.linspace(6541.7, 6594.417,100),\n",
    "               ('Ti','O'): np.linspace(4969.0, 5021.024,100)}\n",
    "\n",
    "colors_by_pair = {('Ti','O'):'orangered',\n",
    "                  ('V','O'):'darkorange',\n",
    "                  ('Cr','O'):'gold',\n",
    "                  ('Mn','O'):'seagreen',\n",
    "                  ('Fe','O'):'dodgerblue',\n",
    "                  ('Co','O'):'navy',\n",
    "                  ('Ni','O'):'rebeccapurple',\n",
    "                  ('Cu','O'):\"mediumvioletred\"}\n",
    "\n",
    "pair_to_name={'Ti':\"Titanium\",'V':'Vanadium',\n",
    "              'Cr':'Chromium','Mn':\"Manganese\",\n",
    "              'Fe':\"Iron\",'Co':\"Cobalt\",\n",
    "             'Ni':'Nickel','Cu':'Copper'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_outliers(mu):\n",
    "    \"\"\"\n",
    "    Given an absorption spectra, check on three unphysical ways we saw FEFF spectra behave for a small\n",
    "    number of outlying data sets:\n",
    "    1. A strange absorption pattern in which the absorption increases, rather than decays, in the XAFS region\n",
    "    2. Unusually high absorption value\n",
    "    3. A spuriously high peak-- larger than the edge's peak-- in the pre-edge region.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if mu[-1]==np.max(mu):\n",
    "        return False\n",
    "    \n",
    "    if np.max(mu)>3:\n",
    "        return False\n",
    "    \n",
    "    if np.max(mu) in mu[:10]:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def test_poly_error_fit_N(X_set, idxs, x_domain, error_bound=.1, use_norm = True):\n",
    "    \"\"\"\n",
    "    Run one-shot fitting over a large variety of X values.\n",
    "    \"\"\"\n",
    "    idx_keep = []\n",
    "    \n",
    "    for spec,idx in zip(X_set,idxs):\n",
    "        \n",
    "        within_bound = one_shot_poly_error_fit(x_domain,spec,\n",
    "                                               use_norm=use_norm,\n",
    "                                               error_bound=error_bound)\n",
    "        \n",
    "        if not within_bound:\n",
    "            continue\n",
    "        idx_keep.append(idx)\n",
    "            \n",
    "    return idx_keep, len(idxs)-len(idx_keep)\n",
    "\n",
    "def one_shot_poly_error_fit(x_domain,mu,use_norm = True, error_bound = .1):\n",
    "    \"\"\"\n",
    "    Pass in indexes to test of a set of polynomials, and test to see if\n",
    "    the polynomials fit to within a certain error. \n",
    "    \"\"\"\n",
    "    \n",
    "    if use_norm:\n",
    "        poly_set = polynomialize_by_idx(x_domain,mu/np.max(mu),N=20,deg=3,label_type='frac')\n",
    "    else:\n",
    "        poly_set = polynomialize_by_idx(x_domain,mu,N=20,deg=3,label_type='frac')\n",
    "    \n",
    "    for poly in poly_set:\n",
    "            poly.error = gauge_polynomial_error(poly.x,poly.y,poly,error='abs')\n",
    "            if poly.error>.1:\n",
    "                return False\n",
    "\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Pointwise Data; \n",
    "### Prune points based on undesirable X or Y features and record.\n",
    "\n",
    "Criteria are:\n",
    "1. If coordination is not 4,5,6 AND no Bader charge info is available:\n",
    "2. If mu contains abnormally high white-line absorption, if there are spuriously large pre-edge peaks, or if there are spuriously large absorption values for large energy values.\n",
    "3. If the finest polynomials cannot fit to the spectrum above a cutoff fidelity.\n",
    "\n",
    "\"Ineligible specta are those with missing bader charge, or coordination not in [4,5,6].\n",
    "\"Outliered\" spectra are those which we gauged to be unphysical due to the criteria described above.\n",
    "\"Unfeaturized\" spectra are those which 20-fold cubic polynonmial fits perform poorly on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T20:31:51.168900Z",
     "start_time": "2019-10-18T20:31:41.692308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de96e96f9dc403c95eedd556c09eb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading in data', max=8.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ti', 'O'): Raw total:4930, Kept: 4793, Ineligible=57, Outliered=26, Unfeaturized= 54,\n",
      "('V', 'O'): Raw total:7120, Kept: 6929, Ineligible=30, Outliered=12, Unfeaturized= 149,\n",
      "('Cr', 'O'): Raw total:2542, Kept: 2395, Ineligible=8, Outliered=18, Unfeaturized= 121,\n",
      "('Mn', 'O'): Raw total:8504, Kept: 7917, Ineligible=42, Outliered=504, Unfeaturized= 41,\n",
      "('Fe', 'O'): Raw total:7362, Kept: 6744, Ineligible=25, Outliered=523, Unfeaturized= 70,\n",
      "('Co', 'O'): Raw total:3533, Kept: 3453, Ineligible=7, Outliered=66, Unfeaturized= 7,\n",
      "('Ni', 'O'): Raw total:3420, Kept: 3396, Ineligible=11, Outliered=12, Unfeaturized= 1,\n",
      "('Cu', 'O'): Raw total:3496, Kept: 3444, Ineligible=21, Outliered=31, Unfeaturized= 0,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_by_pair = {pair:[] for pair in target_elements_groups}\n",
    "for pair in tqdm(target_elements_groups,desc='Loading in data'):\n",
    "    target_file = storage_directory + '/{}_XY.json'.format(pair[0])\n",
    "    cur_x = x_domains[pair]\n",
    "    with open(target_file, 'r') as f:\n",
    "        raw_count = 0\n",
    "        ineligible = 0\n",
    "        outliered = 0\n",
    "        unfeaturized = 0 \n",
    "        for line in f.readlines():\n",
    "            cur_data = json.loads(line)\n",
    "            raw_count+=1  \n",
    "           \n",
    "            if not (cur_data.get('coordination') in [4,5,6] or cur_data.get('bader')):\n",
    "                ineligible +=1\n",
    "                continue\n",
    "            \n",
    "            if not prune_outliers(cur_data['mu']):\n",
    "                outliered +=1\n",
    "                continue\n",
    "                \n",
    "            if not one_shot_poly_error_fit(x_domains[pair],cur_data['mu']):\n",
    "                unfeaturized +=1\n",
    "                continue\n",
    "            \n",
    "            data_by_pair[pair].append(cur_data)\n",
    "            #print(cur_data['metadata'])\n",
    "            \n",
    "    print(f\"{pair}: Raw total:{raw_count}, Kept: {len(data_by_pair[pair])}, Ineligible={ineligible},\"\n",
    "          f\" Outliered={outliered}, Unfeaturized= {unfeaturized},\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up train/test sets \n",
    "##  (using indices to allow for easier data augmentation if desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ti', 'O') Coordination Data Points: 4709 ['4:334', '5:2301', '6:2074']\n",
      "('Ti', 'O') Bader Data Points: 3201\n",
      "('Ti', 'O') Nearest Neighbor Data Points Used: 4709 / 84\n",
      "('Ti', 'O') Coordination Train/Train+Aug/Valid/Test Split: 3814/5514/424/471\n",
      "-----------\n",
      "('V', 'O') Coordination Data Points: 6862 ['4:1954', '5:2404', '6:2504']\n",
      "('V', 'O') Bader Data Points: 3863\n",
      "('V', 'O') Nearest Neighbor Data Points Used: 6862 / 67\n",
      "('V', 'O') Coordination Train/Train+Aug/Valid/Test Split: 5557/6063/618/687\n",
      "-----------\n",
      "('Cr', 'O') Coordination Data Points: 2342 ['4:436', '5:580', '6:1326']\n",
      "('Cr', 'O') Bader Data Points: 1809\n",
      "('Cr', 'O') Nearest Neighbor Data Points Used: 2342 / 53\n",
      "('Cr', 'O') Coordination Train/Train+Aug/Valid/Test Split: 1896/3213/211/235\n",
      "-----------\n",
      "('Mn', 'O') Coordination Data Points: 7810 ['4:302', '5:3873', '6:3635']\n",
      "('Mn', 'O') Bader Data Points: 4031\n",
      "('Mn', 'O') Nearest Neighbor Data Points Used: 7810 / 107\n",
      "('Mn', 'O') Coordination Train/Train+Aug/Valid/Test Split: 6326/9453/703/781\n",
      "-----------\n",
      "('Fe', 'O') Coordination Data Points: 6673 ['4:1052', '5:3087', '6:2534']\n",
      "('Fe', 'O') Bader Data Points: 3908\n",
      "('Fe', 'O') Nearest Neighbor Data Points Used: 6673 / 71\n",
      "('Fe', 'O') Coordination Train/Train+Aug/Valid/Test Split: 5404/7557/601/668\n",
      "-----------\n",
      "('Co', 'O') Coordination Data Points: 3436 ['4:538', '5:1047', '6:1851']\n",
      "('Co', 'O') Bader Data Points: 2075\n",
      "('Co', 'O') Nearest Neighbor Data Points Used: 3436 / 17\n",
      "('Co', 'O') Coordination Train/Train+Aug/Valid/Test Split: 2782/4512/310/344\n",
      "-----------\n",
      "('Ni', 'O') Coordination Data Points: 3361 ['4:206', '5:972', '6:2183']\n",
      "('Ni', 'O') Bader Data Points: 2224\n",
      "('Ni', 'O') Nearest Neighbor Data Points Used: 3361 / 35\n",
      "('Ni', 'O') Coordination Train/Train+Aug/Valid/Test Split: 2721/5310/303/337\n",
      "-----------\n",
      "('Cu', 'O') Coordination Data Points: 3376 ['4:468', '5:2257', '6:651']\n",
      "('Cu', 'O') Bader Data Points: 2167\n",
      "('Cu', 'O') Nearest Neighbor Data Points Used: 3376 / 68\n",
      "('Cu', 'O') Coordination Train/Train+Aug/Valid/Test Split: 2734/5502/304/338\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "ttc_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "ttb_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "ttnn_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "ttmd_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "\n",
    "\n",
    "for pair in target_elements_groups:\n",
    "    \n",
    "    np.random.seed(rseed)\n",
    "\n",
    "    X_c = []; X_c_st =[] ; X_c_sq = [];\n",
    "    X_c_p1 = []; X_c_m1 = []; M_c = []\n",
    "    Y_c = []\n",
    "    \n",
    "    # Bader\n",
    "    X_b = [];\n",
    "    X_b_st = []; X_b_sq = []; \n",
    "    X_b_p1 = []; X_b_m1 = []\n",
    "\n",
    "    Y_b = [] # bader value\n",
    "    \n",
    "    # NN dist max - min\n",
    "    X_nn = [];\n",
    "    X_nn_st = []; X_nn_sq = []; \n",
    "    X_nn_p1 = []; X_nn_m1 = []\n",
    "    Y_nn = []; Y_md = [] \n",
    "    \n",
    "    coord_tally = [bool(point.get('coordination') in [4,5,6]) for point in data_by_pair[pair]]\n",
    "    bader_tally = [bool(point.get('bader')) for point in data_by_pair[pair]]\n",
    "    \n",
    "    pruned_coord_env    = []\n",
    "    \n",
    "    ############################\n",
    "    # EXTRACT DATA FROM POINTS #\n",
    "    ############################\n",
    "    discounted_md = 0\n",
    "    ineleigible = 0\n",
    "    outliered = 0\n",
    "    unfeaturized = 0\n",
    "    for point in data_by_pair[pair]:\n",
    "            \n",
    "        if point.get('coordination') in [4,5,6]:\n",
    "            if use_max_normalized:\n",
    "                \n",
    "                X_c.append(point['mu_norm'])\n",
    "                \n",
    "            else:\n",
    "                X_c.append(point['mu'])\n",
    "            \n",
    "            Y_c.append(point['coordination'])\n",
    "            M_c.append(point['metadata'])\n",
    "            \n",
    "        if point.get('bader'):\n",
    "            \n",
    "\n",
    "            if use_max_normalized:\n",
    "                X_b.append(point['mu_norm'])\n",
    "            else:\n",
    "                X_b.append(point['mu'])\n",
    "\n",
    "            Y_b.append(point['bader'])       \n",
    "                \n",
    "        if point.get('nn_min-max') is not None and point.get('coordination') in [4,5,6]:\n",
    "                \n",
    "            if use_max_normalized:\n",
    "                X_nn.append( point['mu_norm'])\n",
    "                \n",
    "                X_nn_st.append(point['mu_stretch']/np.max(point['mu_stretch']))\n",
    "                X_nn_sq.append(point['mu_squeeze']/np.max(point['mu_squeeze']))\n",
    "                \n",
    "                X_nn_p1.append(point['mu_p1']/np.max(point['mu_p1']))\n",
    "                X_nn_m1.append(point['mu_m1']/np.max(point['mu_m1']))\n",
    "            else:\n",
    "                X_nn.append(point['mu'])\n",
    "                \n",
    "                X_nn_st.append(point['mu_stretch'])\n",
    "                X_nn_sq.append(point['mu_squeeze'])\n",
    "                \n",
    "                X_nn_p1.append(point['mu_p1'])\n",
    "                X_nn_m1.append(point['mu_m1'])\n",
    "            \n",
    "            Y_nn.append(point['nn_min-max'])    \n",
    "            Y_md.append(point['avg_nn_dists'])\n",
    "            \n",
    "        elif point.get('nn_min-max') is not None:\n",
    "            discounted_md += 1\n",
    "        \n",
    "    \n",
    "    print(\"{} Coordination Data Points:\".format(pair),len(Y_c), [str(x)+\":\"+str(Counter(Y_c)[x]) for x in sorted(Counter(Y_c).keys())])\n",
    "    print(\"{} Bader Data Points:\".format(pair),len(Y_b))\n",
    "    print(\"{} Nearest Neighbor Data Points Used:\".format(pair),len(Y_md),'/',discounted_md)\n",
    "    \n",
    "    all_coord_indices = [n for n in range(len(Y_c))]\n",
    "    all_bader_indices = [n for n in range(len(Y_b))]\n",
    "    \n",
    "    X_c = np.array(X_c);  \n",
    "    X_c_p1 = np.array(X_c_p1); X_c_m1 = np.array(X_c_m1)\n",
    "    X_c_sq = np.array(X_c_sq); X_c_st = np.array(X_c_st)\n",
    "    Y_c = np.array(Y_c)\n",
    "\n",
    "\n",
    "    X_b = np.array(X_b); \n",
    "    X_b_p1 = np.array(X_b_p1) ;X_b_m1 = np.array(X_b_m1)\n",
    "    X_b_sq = np.array(X_b_sq); X_b_st = np.array(X_b_st)\n",
    "    Y_b = np.array(Y_b)\n",
    "  \n",
    "    X_nn = np.array(X_nn)    \n",
    "    X_nn_p1 = np.array(X_nn_p1); X_nn_m1 = np.array(X_nn_m1)\n",
    "    X_nn_st = np.array(X_nn_st); X_nn_sq = np.array(X_nn_sq)\n",
    "    Y_nn = np.array(Y_nn); Y_md = np.array(Y_md)\n",
    "\n",
    "    \n",
    "    ##########################\n",
    "    # COORDINATION SETUP\n",
    "    ##########################\n",
    "    c_train_idx , c_test_idx, _, _ = \\\n",
    "          train_test_split(all_coord_indices,all_coord_indices, test_size=0.1,\n",
    "                   random_state=rseed)\n",
    "    c_train_idx , c_valid_idx, _, _ = \\\n",
    "          train_test_split(c_train_idx,c_train_idx, test_size=0.1,\n",
    "                   random_state=rseed)\n",
    "    \n",
    "    if drop_poly_problems:\n",
    "        c_train_idx, c_drop_train = test_poly_error_fit_N(X_c,c_train_idx,x_domains[pair])\n",
    "        c_valid_idx, c_drop_valid = test_poly_error_fit_N(X_c,c_valid_idx,x_domains[pair])\n",
    "        c_test_idx, c_drop_test  = test_poly_error_fit_N(X_c,c_test_idx, x_domains[pair])\n",
    "    \n",
    "    c_squeeze_idx = np.random.choice(c_train_idx,size=int(.3*len(c_train_idx)),replace=False)\n",
    "    c_stretch_idx = np.random.choice(c_train_idx,size=int(.3*len(c_train_idx)),replace=False)\n",
    "    \n",
    "    xc_train = X_c[c_train_idx]; yc_train = Y_c[c_train_idx]\n",
    "    xc_valid = X_c[c_valid_idx]; yc_valid = Y_c[c_valid_idx]\n",
    "    xc_test = X_c[c_test_idx]  ; yc_test = Y_c[c_test_idx]\n",
    "    pre_aug = len(yc_train)\n",
    "    \n",
    "\n",
    "    \n",
    "    ros = RandomOverSampler(random_state = rseed)\n",
    "    if oversampling:\n",
    "        xc_train, yc_train = ros.fit_resample(xc_train,yc_train)\n",
    "    \n",
    "    \n",
    "    ttc_by_pair[pair]['train_x'] = xc_train \n",
    "    ttc_by_pair[pair]['train_y'] = yc_train\n",
    "    \n",
    "    ttc_by_pair[pair]['valid_x'] = xc_valid\n",
    "    ttc_by_pair[pair]['valid_y'] = yc_valid\n",
    "    \n",
    "    ttc_by_pair[pair]['test_x'] = xc_test\n",
    "    ttc_by_pair[pair]['test_y'] = yc_test\n",
    "    ttc_by_pair[pair]['valid_metadata'] = np.array(M_c)[c_valid_idx]\n",
    "    \n",
    "    print(\"{} Coordination Train/Train+Aug/Valid/Test Split: {}/{}/{}/{}\".format(pair,pre_aug,len(yc_train),len(yc_valid),len(yc_test)))\n",
    "\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    # BADER SETUP\n",
    "    #############################################\n",
    "    \n",
    "    b_train_idx , b_test_idx, _, _ = \\\n",
    "          train_test_split(all_bader_indices,all_bader_indices, test_size=0.1,\n",
    "                   random_state=rseed)\n",
    "    b_train_idx , b_valid_idx, _, _ = \\\n",
    "          train_test_split(b_train_idx,b_train_idx, test_size=0.1,\n",
    "                   random_state=rseed)\n",
    "    \n",
    "    if drop_poly_problems:\n",
    "        \n",
    "        b_train_idx, drop_train = test_poly_error_fit_N(X_b,b_train_idx,x_domains[pair])\n",
    "        b_valid_idx, drop_valid = test_poly_error_fit_N(X_b,b_valid_idx,x_domains[pair])\n",
    "        b_test_idx, drop_test = test_poly_error_fit_N(X_b,b_test_idx, x_domains[pair])\n",
    "        #print(f\"Dropped for {pair} bader: {drop_train+drop_valid+drop_test}\")\n",
    "\n",
    "    \n",
    "    b_squeeze_idx = np.random.choice(b_train_idx,size=int(.3*len(b_train_idx)),replace=False)\n",
    "    b_stretch_idx = np.random.choice(b_train_idx,size=int(.3*len(b_train_idx)),replace=False)\n",
    "    \n",
    "    yb_train = Y_b[b_train_idx]\n",
    "    yb_valid = Y_b[b_valid_idx]\n",
    "    yb_test  = Y_b[b_test_idx]\n",
    "\n",
    "    xb_train = X_b[b_train_idx]\n",
    "    xb_valid = X_b[b_valid_idx]\n",
    "    xb_test  = X_b[b_test_idx]  \n",
    "    \n",
    "    if data_aug:\n",
    "        xb_train = np.vstack((xb_train, \n",
    "                              X_b_p1[b_train_idx],\n",
    "                              X_b_m1[b_train_idx], \n",
    "                              X_b_sq[b_squeeze_idx],\n",
    "                              X_b_st[b_stretch_idx]))\n",
    "        yb_train = np.array(list(yb_train)+ \\\n",
    "                            list(Y_b[b_train_idx])+ \\\n",
    "                            list(Y_b[b_train_idx])+ \\\n",
    "                            list(Y_b[b_squeeze_idx])+ \\\n",
    "                            list(Y_b[b_stretch_idx]))\n",
    "\n",
    "        \n",
    "    assert xb_train.shape[0] == len(yb_train)\n",
    "    \n",
    "    ttb_by_pair[pair]['train_x'] = xb_train \n",
    "    ttb_by_pair[pair]['train_y'] = yb_train\n",
    "    \n",
    "    ttb_by_pair[pair]['valid_x'] = xb_valid\n",
    "    ttb_by_pair[pair]['valid_y'] = yb_valid\n",
    "    \n",
    "    ttb_by_pair[pair]['test_x'] = xb_test\n",
    "    ttb_by_pair[pair]['test_y'] = yb_test\n",
    "    \n",
    "    \n",
    "    ######################################################\n",
    "    # NEIGHBOR PART\n",
    "    ######################################################\n",
    "    assert (len(Y_md)==len(Y_nn))\n",
    "    all_nbr_indices = [n for n in range(len(Y_md))]\n",
    "\n",
    "    nbr_train_idx , nbr_test_idx, _, _ = \\\n",
    "      train_test_split(all_nbr_indices,all_nbr_indices, test_size=0.1,\n",
    "               random_state=rseed)\n",
    "    nbr_train_idx , nbr_valid_idx, _, _ = \\\n",
    "      train_test_split(nbr_train_idx,nbr_train_idx, test_size=0.1,\n",
    "                   random_state=rseed)\n",
    "    \n",
    "    if drop_poly_problems:\n",
    "        \n",
    "        nbr_train_idx, drop_train = test_poly_error_fit_N(X_nn,nbr_train_idx,x_domains[pair])\n",
    "        nbr_valid_idx, drop_valid = test_poly_error_fit_N(X_nn,nbr_valid_idx,x_domains[pair])\n",
    "        nbr_test_idx, drop_test  = test_poly_error_fit_N(X_nn,nbr_test_idx, x_domains[pair])\n",
    "        #print(f\"Dropped for {pair} mean: {drop_train+drop_valid+drop_test}\")\n",
    "\n",
    "    nn_squeeze_idx = np.random.choice(nbr_train_idx,size=int(.3*len(nbr_train_idx)),replace=False)\n",
    "    nn_stretch_idx = np.random.choice(nbr_train_idx,size=int(.3*len(nbr_train_idx)),replace=False)\n",
    "    \n",
    "\n",
    "    # NEAREST NEIGHBOR DISTANCE MAX - MIN , UN NORMALIZED\n",
    "    \n",
    "    xnn_train = X_nn[nbr_train_idx]\n",
    "    xnn_valid = X_nn[nbr_valid_idx]\n",
    "    xnn_test = X_nn[nbr_test_idx]\n",
    "\n",
    "    ymd_train = Y_md[nbr_train_idx]\n",
    "    ymd_valid = Y_md[nbr_valid_idx]\n",
    "    ymd_test  = Y_md[nbr_test_idx]\n",
    "    \n",
    "    assert xnn_train.shape[0] == len(ymd_train)\n",
    "    \n",
    "    if data_aug:\n",
    "        xnn_train = np.vstack((xnn_train, \n",
    "                              X_nn_p1[nbr_train_idx],\n",
    "                              X_nn_m1[nbr_train_idx], \n",
    "                               X_nn_sq[nn_squeeze_idx],\n",
    "                              X_nn_st[nn_stretch_idx]))\n",
    "        ymd_train = np.array(list(ymd_train)\n",
    "                             + list(Y_md[nbr_train_idx])\n",
    "                             + list(Y_md[nbr_train_idx])\n",
    "                             + list(Y_md[nn_squeeze_idx])\n",
    "                             +  list(Y_md[nn_stretch_idx]))\n",
    "    \n",
    "    \n",
    "    assert xnn_train.shape[0] == len(ymd_train), str(xnn_train.shape) + str(len(ymd_train))\n",
    "    \n",
    "    ttnn_by_pair[pair]['train_x'] = xnn_train \n",
    "    ttnn_by_pair[pair]['valid_x'] = xnn_valid\n",
    "    ttnn_by_pair[pair]['test_x'] = xnn_test\n",
    "    \n",
    "    # MEAN NEIGHBOR DISTANCE \n",
    "\n",
    "    ttmd_by_pair[pair]['train_y'] = ymd_train\n",
    "    ttmd_by_pair[pair]['valid_y'] = ymd_valid\n",
    "    ttmd_by_pair[pair]['test_y'] = ymd_test\n",
    "    print('-----------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./model_data'):\n",
    "    os.makedirs('./model_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in target_elements_groups:\n",
    "    for key in ['train_x','train_y','valid_x','valid_y','test_x','test_y']:\n",
    "        np.save(f'./model_data/{pair[0]}_coord_{key}.npy',ttc_by_pair[pair][key],allow_pickle=False)\n",
    "        np.save(f'./model_data/{pair[0]}_bader_{key}.npy',ttb_by_pair[pair][key],allow_pickle=False)\n",
    "        if 'x' in key:\n",
    "            np.save(f'./model_data/{pair[0]}_md_{key}.npy',ttnn_by_pair[pair][key],allow_pickle=False)\n",
    "        else:\n",
    "            np.save(f'./model_data/{pair[0]}_md_{key}.npy',ttmd_by_pair[pair][key],allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Polynomial Fit Data\n",
    "\n",
    "Note: Run the following cells twice, once in both feff and max normalized mode, to write both data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_max_normalized = False\n",
    "norm_str = 'max' if use_max_normalized else 'feff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e3a82b01b34b09af82d2030257ac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading in data', max=8.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "poly_data_by_pair = {pair:[] for pair in target_elements_groups}\n",
    "for pair in tqdm(target_elements_groups,desc='Loading in data'):\n",
    "    if use_max_normalized: \n",
    "        target_file = storage_directory + '/{}_maxnorm_polynomial_XY.json'.format(pair[0])\n",
    "    else: \n",
    "        target_file = storage_directory + '/{}_feffnorm_polynomial_XY.json'.format(pair[0])\n",
    "    with open(target_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            cur_data = json.loads(line)\n",
    "            if cur_data.get('one_hot_coord') or cur_data.get('bader') or cur_data.get('nn_min-max'):\n",
    "                poly_data_by_pair[pair].append(cur_data)\n",
    "sorted_keys = sorted(list(poly_data_by_pair[pair][0]['labeled_coefficients'].keys()))\n",
    "sorted_keys_filtered = [key for key in sorted_keys if ('fraction_size:1,' not in key and 'fraction_size:2,' not in key and 'random' not in key)]\n",
    "sorted_keys = sorted_keys_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ti', 'O') Coordination Data Points: 4709\n",
      "('Ti', 'O') Bader Data Points: 3201\n",
      "('V', 'O') Coordination Data Points: 6862\n",
      "('V', 'O') Bader Data Points: 3863\n",
      "('Cr', 'O') Coordination Data Points: 2342\n",
      "('Cr', 'O') Bader Data Points: 1809\n",
      "('Mn', 'O') Coordination Data Points: 7810\n",
      "('Mn', 'O') Bader Data Points: 4031\n",
      "('Fe', 'O') Coordination Data Points: 6673\n",
      "('Fe', 'O') Bader Data Points: 3908\n",
      "('Co', 'O') Coordination Data Points: 3436\n",
      "('Co', 'O') Bader Data Points: 2075\n",
      "('Ni', 'O') Coordination Data Points: 3361\n",
      "('Ni', 'O') Bader Data Points: 2224\n",
      "('Cu', 'O') Coordination Data Points: 3376\n",
      "('Cu', 'O') Bader Data Points: 2167\n"
     ]
    }
   ],
   "source": [
    "pttc_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "pttb_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "pttmd_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "\n",
    "for pair in target_elements_groups:\n",
    "\n",
    "    X_pc = []\n",
    "    Y_pc = []\n",
    "    \n",
    "    X_pmd = []\n",
    "    Y_pmd = []\n",
    "    \n",
    "    X_pb = []\n",
    "    Y_pb = []\n",
    "    \n",
    "    for point in poly_data_by_pair[pair]:\n",
    "        if point.get('coordination') in [4,5,6]:\n",
    "            \n",
    "            X_pc.append([point['labeled_coefficients'][key] for key in sorted_keys])\n",
    "            Y_pc.append(point['coordination'])\n",
    "    \n",
    "        if point.get('coordination') in [4,5,6] and point.get('avg_nn_dists') is not None:\n",
    "            X_pmd.append([point['labeled_coefficients'][key] for key in sorted_keys])\n",
    "            Y_pmd.append(point['avg_nn_dists'])\n",
    "\n",
    "        if point.get('bader') is not None:\n",
    "            X_pb.append([point['labeled_coefficients'][key] for key in sorted_keys])\n",
    "            Y_pb.append(point['bader'])            \n",
    "\n",
    "    assert len(Y_pmd)==len(X_pmd) and len(X_pc)==len(Y_pc) and len(X_pb)==len(Y_pb)\n",
    "    print(\"{} Coordination Data Points:\".format(pair),len(Y_pc))\n",
    "    print(\"{} Bader Data Points:\".format(pair),len(Y_pb))\n",
    "\n",
    "    X_pc = np.array(X_pc)\n",
    "    X_pb = np.array(X_pb)\n",
    "    X_pmd = np.array(X_pmd)\n",
    "    \n",
    "    Y_pc = np.array(Y_pc)\n",
    "    Y_pb = np.array(Y_pb)\n",
    "    Y_pmd = np.array(Y_pmd)\n",
    "\n",
    "    # COORDINATION ON POLYNOMIALS\n",
    "    xc_train, xc_test, yc_train, yc_test = \\\n",
    "      train_test_split(X_pc, Y_pc, test_size=0.1,\n",
    "                   random_state=rseed)\n",
    "    xc_train, xc_valid, yc_train, yc_valid = \\\n",
    "      train_test_split(xc_train, yc_train, test_size=0.1,\n",
    "                       random_state=rseed)\n",
    "        \n",
    "    pttc_by_pair[pair]['train_x'] = xc_train \n",
    "    pttc_by_pair[pair]['train_y'] = yc_train\n",
    "    pttc_by_pair[pair]['valid_x'] = xc_valid\n",
    "    pttc_by_pair[pair]['valid_y'] = yc_valid\n",
    "    pttc_by_pair[pair]['test_x']  = xc_test\n",
    "    pttc_by_pair[pair]['test_y']  = yc_test\n",
    "    \n",
    "    \n",
    "    # BADER CHARGES ON POLYNOMIALS\n",
    "    xbn_train, xbn_test, ybn_train, ybn_test = \\\n",
    "      train_test_split(X_pb, Y_pb, test_size=0.1,\n",
    "                   random_state=rseed)\n",
    "    xbn_train, xbn_valid, ybn_train, ybn_valid = \\\n",
    "      train_test_split(xbn_train, ybn_train, test_size=0.1,\n",
    "                       random_state=rseed)\n",
    "    \n",
    "    pttb_by_pair[pair]['train_x'] = xbn_train \n",
    "    pttb_by_pair[pair]['train_y'] = ybn_train\n",
    "    pttb_by_pair[pair]['valid_x'] = xbn_valid\n",
    "    pttb_by_pair[pair]['valid_y'] = ybn_valid\n",
    "    pttb_by_pair[pair]['test_x']  = xbn_test\n",
    "    pttb_by_pair[pair]['test_y']  = ybn_test\n",
    "    \n",
    "    \n",
    "    # Max - Min distance ON POLYNOMIALS\n",
    "    \n",
    "    all_md_idx = [n for n in range(len(X_pmd))]\n",
    "    \n",
    "    md_train_idx, md_test_idx , _, _ = \\\n",
    "            train_test_split(all_md_idx, all_md_idx, test_size=0.1,\n",
    "                               random_state=rseed)\n",
    "    md_train_idx, md_valid_idx, _, _ = \\\n",
    "        train_test_split(md_train_idx, md_train_idx, test_size=0.1,\n",
    "                                   random_state=rseed)\n",
    "    \n",
    "    xmd_train = X_pmd[md_train_idx]\n",
    "    xmd_valid = X_pmd[md_valid_idx]\n",
    "    xmd_test  = X_pmd[md_test_idx]\n",
    "    \n",
    "    ymd_train = Y_pmd[md_train_idx]\n",
    "    ymd_valid = Y_pmd[md_valid_idx]\n",
    "    ymd_test  = Y_pmd[md_test_idx]\n",
    "    \n",
    "    \n",
    "    pttmd_by_pair[pair]['train_x'] = xmd_train \n",
    "    pttmd_by_pair[pair]['train_y'] = ymd_train\n",
    "    \n",
    "    pttmd_by_pair[pair]['valid_x'] = xmd_valid\n",
    "    pttmd_by_pair[pair]['valid_y'] = ymd_valid\n",
    "    \n",
    "    pttmd_by_pair[pair]['test_x']  = xmd_test    \n",
    "    pttmd_by_pair[pair]['test_y']  = ymd_test    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write polynomial model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in target_elements_groups:\n",
    "    for key in ['train_x','train_y','valid_x','valid_y','test_x','test_y']:\n",
    "        np.save(f'./model_data/{pair[0]}_{norm_str}norm_polynomial_coord_{key}.npy',pttc_by_pair[pair][key],allow_pickle=False)\n",
    "        np.save(f'./model_data/{pair[0]}_{norm_str}norm_polynomial_bader_{key}.npy',pttb_by_pair[pair][key],allow_pickle=False)\n",
    "        np.save(f'./model_data/{pair[0]}_{norm_str}norm_polynomial_md_{key}.npy',pttmd_by_pair[pair][key],allow_pickle=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
